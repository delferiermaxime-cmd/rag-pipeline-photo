# RAG Local

Application RAG (**Retrieval-Augmented Generation**) auto-hÃ©bergÃ©e permettant d'interroger vos documents via un LLM local. **Aucune donnÃ©e ne quitte votre serveur.**

> ğŸ”“ Stack 100% open source â€” auto-hÃ©bergeable, aucune dÃ©pendance cloud.

---

## Table des matiÃ¨res

- [Architecture](#architecture)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Mise Ã  jour](#mise-Ã -jour)
- [Utilisation](#utilisation)
- [ParamÃ¨tres avancÃ©s](#paramÃ¨tres-avancÃ©s)
- [RÃ©solution d'erreurs](#rÃ©solution-derreurs)

---

## Architecture

```
Browser â†’ Nginx (port 80)
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Frontend        â”‚  Next.js 15 (port 3000)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ /api/*
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Backend        â”‚  FastAPI (port 8000)
    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚      â”‚       â”‚
  â”Œâ”€â”€â”€â”€â–¼â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Ollamaâ”‚ â”‚Qdrantâ”‚  â”‚Postgresâ”‚
  â”‚LLM + â”‚ â”‚Vect. â”‚  â”‚Users + â”‚
  â”‚Embed.â”‚ â”‚Base  â”‚  â”‚Docs +  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜  â”‚Histor. â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pipeline RAG :**
```
Fichier â†’ Docling â†’ Markdown â†’ Chunks (3000 chars, overlap 450) â†’ bge-m3 â†’ Qdrant
Question â†’ bge-m3 â†’ Qdrant MMR (similaritÃ© cosine + diversitÃ©) â†’ TOP_K chunks â†’ LLM â†’ RÃ©ponse SSE
```

**Stack technique :**

| Composant | Technologie | Licence | Lien |
|---|---|---|---|
| Frontend | Next.js 15, TypeScript | MIT | [docs](https://nextjs.org/docs) |
| Backend | FastAPI, SQLAlchemy async | MIT | [docs](https://fastapi.tiangolo.com) |
| Base vectorielle | Qdrant | Apache 2.0 | [docs](https://qdrant.tech/demo/) |
| Base de donnÃ©es | PostgreSQL 16 | PostgreSQL License | [docs](https://www.postgresql.org/docs/16/index.html) |
| LLM & Embedding | Ollama (gemma3, deepseek-r1, bge-m3) | MIT | [docs](https://docs.ollama.com) |
| Parser de documents | Docling (OCR, tableaux, images) | MIT | [docs](https://www.docling.ai) |
| Reverse proxy | Nginx | BSD 2-Clause | [docs](https://nginx.org) |

> âœ… Usage commercial autorisÃ© pour l'ensemble de la stack. Voir licences individuelles pour les conditions dÃ©taillÃ©es. Les modÃ¨les LLM ont leurs propres licences â€” vÃ©rifier sur [ollama.com/library](https://ollama.com/library).

---

## PrÃ©requis

- **OS** : Linux (Ubuntu 22.04+ recommandÃ©)
- **RAM** : 8 GB minimum, 16 GB recommandÃ© (selon le modÃ¨le LLM)
- **Stockage** : 20 GB minimum (modÃ¨les LLM inclus)
- **Docker** : 24.0+
- **Docker Compose** : 2.20+
- **Git**

VÃ©rifier les versions :
```bash
docker --version
docker compose version
git --version
```

---

## Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/delferiermaxime-cmd/rag-pipeline-photo.git
cd rag-pipeline-photo
```

### 2. Configurer les variables d'environnement

```bash
cp .env.example .env
nano .env
```


```bash
cd frontend

# RegÃ©nÃ©rer le package-lock.json
npm install

# Revenir Ã  la racine et rebuilder
cd ..
```



Contenu minimal du `.env` :
```env
SECRET_KEY=changez-moi-avec-une-cle-tres-longue-et-aleatoire
CORS_ORIGINS=http://localhost,http://votre-ip-serveur
```

GÃ©nÃ©rer une clÃ© secrÃ¨te sÃ©curisÃ©e :
```bash
python3 -c "import secrets; print(secrets.token_hex(32))"
```

### 3. Build et dÃ©marrage

```bash
docker compose build --no-cache
docker compose up -d
```

> â± Le premier build prend **10-15 minutes** (tÃ©lÃ©chargement de Docling et ses modÃ¨les).

### 4. TÃ©lÃ©charger les modÃ¨les Ollama

```bash
# ModÃ¨le LLM (obligatoire â€” choisir selon la RAM disponible)
docker compose exec ollama ollama pull gemma3:4b      # ~3 GB â€” 8 GB RAM
docker compose exec ollama ollama pull gemma3:12b     # ~8 GB â€” 16 GB RAM
docker compose exec ollama ollama pull deepseek-r1:14b # ~9 GB â€” 16 GB RAM

# ModÃ¨le d'embedding (obligatoire)
docker compose exec ollama ollama pull bge-m3:567m    # ~600 MB
```

### 5. VÃ©rifier que tout fonctionne

```bash
docker compose ps
```

Tous les containers doivent Ãªtre en Ã©tat `Up` :
```
rag-pipeline-photo-backend-1    Up
rag-pipeline-photo-frontend-1   Up
rag-pipeline-photo-nginx-1      Up
rag-pipeline-photo-ollama-1     Up
rag-pipeline-photo-postgres-1   Up
rag-pipeline-photo-qdrant-1     Up
```

### 6. AccÃ©der Ã  l'application

Ouvrir **http://votre-ip-serveur** dans le navigateur, crÃ©er un compte et commencer Ã  uploader des documents.

> **AccÃ¨s distant via SSH tunnel :**
> ```bash
> ssh -p PORT user@ip-serveur -L 8080:localhost:80 -N -f
> ```
> Puis ouvrir `http://localhost:8080`

---

## Configuration

### Variables d'environnement (`.env`)

| Variable | Description | DÃ©faut |
|---|---|---|
| `SECRET_KEY` | ClÃ© JWT â€” **obligatoire, changer en production** | â€” |
| `CORS_ORIGINS` | Origines autorisÃ©es (sÃ©parÃ©es par virgules) | `http://localhost` |
| `OLLAMA_AVAILABLE_MODELS` | ModÃ¨les disponibles dans l'UI | `["gemma3:4b"]` |

### ModÃ¨les LLM disponibles

Par dÃ©faut dans `docker-compose.yml` :
```yaml
OLLAMA_AVAILABLE_MODELS: '["gemma3:4b","deepseek-r1:14b","gemma3:12b","gemma3:27b"]'
```

Pour ajouter un modÃ¨le :
```bash
# 1. TÃ©lÃ©charger le modÃ¨le
docker compose exec ollama ollama pull llama3.2:latest

# 2. L'ajouter dans .env (format JSON obligatoire)
OLLAMA_AVAILABLE_MODELS=["gemma3:4b","gemma3:12b","llama3.2:latest"]

# 3. RedÃ©marrer le backend
docker compose restart backend
```

Parcourir tous les modÃ¨les disponibles : **https://ollama.com/library**

### Formats de documents supportÃ©s

| Format | Extension | Parser |
|---|---|---|
| PDF | `.pdf` | Docling + PyPdfium (OCR inclus) |
| Word | `.docx`, `.dotx`, `.doc` | Docling |
| PowerPoint | `.pptx`, `.ppt` | Docling |
| Excel | `.xlsx`, `.xls` | Docling |
| HTML | `.html`, `.htm` | Docling |
| Texte | `.txt`, `.md`, `.csv` | Fallback natif |
| EPUB | `.epub` | Docling |
| AsciiDoc | `.asciidoc`, `.adoc` | Docling |
| ODT/ODS/ODP | `.odt`, `.ods`, `.odp` | Docling |

> âš ï¸ Les fichiers `.dotx` et `.doc` sont automatiquement convertis en `.docx` avant traitement par Docling.

### GPU (optionnel)

Pour utiliser un GPU Nvidia avec Ollama :

```bash
# 1. Installer nvidia-container-toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit

# 2. Configurer Docker
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# 3. VÃ©rifier
nvidia-smi
```

---

## Mise Ã  jour

### Mise Ã  jour complÃ¨te (recommandÃ©e)

```bash
cd ~/rag-pipeline-photo
git pull
docker compose build --no-cache
docker compose up -d
```

> âš ï¸ Le rebuild recompile les images Docker mais **ne supprime pas les donnÃ©es** (PostgreSQL, Qdrant, modÃ¨les Ollama sont dans des volumes persistants).

### Mise Ã  jour rapide du backend (sans rebuild)

```bash
git pull

# Copier les fichiers modifiÃ©s directement dans le container
docker compose up -d backend
sleep 15
docker compose cp backend/app/services/rag_service.py rag-pipeline-photo-backend-1:/app/app/services/rag_service.py
docker compose cp backend/app/routers/chat.py rag-pipeline-photo-backend-1:/app/app/routers/chat.py

docker compose restart backend
```

> âš ï¸ Toujours faire `docker compose up -d backend` **avant** le `cp` â€” le container doit exister pour accepter la copie.

> âš ï¸ Les changements frontend nÃ©cessitent toujours un rebuild complet.

---

## Utilisation

### Uploader des documents

1. Aller dans **Upload**
2. Glisser-dÃ©poser ou cliquer pour parcourir
3. Attendre le statut **âœ… X chunks indexÃ©s**
4. Les documents sont accessibles par **tous les utilisateurs**

> Le systÃ¨me dÃ©tecte automatiquement les **doublons** (mÃªme nom de fichier) et refuse le re-upload.

### GÃ©rer les documents

Aller dans **Documents** pour :
- Rechercher un document par nom
- Supprimer un document individuel (et ses vecteurs associÃ©s)
- **Tout supprimer** en un clic

### Interroger les documents

1. Aller dans **Chat**
2. Taper une question â†’ le LLM rÃ©pond en streaming avec rendu Markdown
3. Les **sources** sont affichÃ©es avec le score de similaritÃ©, la page, et un bouton **"Voir le chunk complet"**
4. Si la rÃ©ponse n'est pas dans les documents, le LLM rÃ©pond depuis ses connaissances gÃ©nÃ©rales

### Filtrer par document

Cliquer sur **"Tous les docs"** dans la toolbar du chat pour :
- SÃ©lectionner un ou plusieurs documents spÃ©cifiques Ã  interroger
- Activer **"ğŸš« Sans base vectorielle"** pour interroger uniquement les connaissances gÃ©nÃ©rales du LLM

### Upload temporaire dans la conversation

Cliquer sur ğŸ“ pour joindre un fichier **sans l'indexer** dans la base vectorielle. Le contenu est injectÃ© directement dans le contexte de la conversation (formats texte uniquement : `.txt`, `.md`, `.csv`, `.html`).

### Historique

Cliquer sur **Historique** pour retrouver les conversations prÃ©cÃ©dentes. Chaque conversation est sauvegardÃ©e automatiquement.

---

## ParamÃ¨tres avancÃ©s

Accessible via **ParamÃ¨tres** dans la sidebar.

| ParamÃ¨tre | Description | DÃ©faut |
|---|---|---|
| **Prompt systÃ¨me** | Instructions donnÃ©es au LLM avant chaque rÃ©ponse | Voir app |
| **TempÃ©rature** | 0 = dÃ©terministe Â· 1 = crÃ©atif | 0.1 |
| **Tokens max** | Longueur maximale de la rÃ©ponse | 1024 |
| **TOP_K** | Nombre de chunks rÃ©cupÃ©rÃ©s depuis Qdrant | 8 |
| **Score minimum** | Seuil de similaritÃ© â€” chunks en dessous ignorÃ©s | 0.3 |
| **Contexte max** | Taille max du contexte envoyÃ© au LLM | 12 000 chars |

> Les paramÃ¨tres sont sauvegardÃ©s localement dans le navigateur.

### Algorithme MMR (diversitÃ© des rÃ©sultats)

Le systÃ¨me utilise **Maximum Marginal Relevance** pour diversifier les sources retournÃ©es. Il rÃ©cupÃ¨re 3Ã— plus de candidats que `TOP_K`, puis sÃ©lectionne les chunks les plus pertinents **et** les plus diversifiÃ©s entre eux â€” Ã©vitant que le mÃªme document occupe tous les slots de rÃ©sultats.

---

## RÃ©solution d'erreurs

### Commandes de diagnostic gÃ©nÃ©rales

```bash
# Ã‰tat de tous les containers
docker compose ps

# Logs backend
docker compose logs backend --tail=50

# Logs frontend
docker compose logs frontend --tail=30

# Tester la connectivitÃ© backend
curl http://localhost/api/v1/auth/me
```

---

### âŒ Bad Gateway au dÃ©marrage

**Cause :** Nginx dÃ©marre avant le backend/frontend.

```bash
docker compose restart nginx
```

Si Ã§a persiste, vÃ©rifier que tous les containers sont `Up` :
```bash
docker compose ps
docker compose logs backend --tail=20
```

---

### âŒ `docker compose cp` Ã©choue â€” `no container found`

**Cause :** Le container n'existe pas encore au moment du `cp`.

```bash
# Toujours dÃ©marrer d'abord, attendre, puis copier
docker compose up -d backend
sleep 15
docker compose cp fichier.py rag-pipeline-photo-backend-1:/app/app/...
docker compose restart backend
```

---

### âŒ Rebuild frontend Ã©choue â€” `npm ci` / `package-lock.json` dÃ©synchronisÃ©

**Cause :** `package.json` modifiÃ© sans regÃ©nÃ©rer le `package-lock.json`.

```bash
# RegÃ©nÃ©rer le lock file sur le serveur
cd ~/rag-pipeline-photo/frontend
npm install
git add package-lock.json
git commit -m "update package-lock.json"
git push
cd ..
docker compose build --no-cache frontend && docker compose up -d frontend
```

---

### âŒ Erreur `.dotx` / `.doc` â€” `Input document is not valid`

**Cause :** Docling valide l'extension du fichier temporaire et rejette `.dotx`/`.doc`.

**Fix :** DÃ©jÃ  corrigÃ© dans `docling_service.py` â€” les fichiers `.dotx`, `.doc`, `.odt` sont automatiquement renommÃ©s en `.docx` avant conversion. Si l'erreur persiste, vÃ©rifier que le fichier `docling_service.py` dans le container est bien la derniÃ¨re version :

```bash
docker compose exec backend grep "DOCX_ALIASES" /app/app/services/docling_service.py
```

---

### âŒ `OLLAMA_AVAILABLE_MODELS` â€” erreur de parsing

**Cause :** Le format doit Ãªtre un tableau JSON valide, pas une liste sÃ©parÃ©e par des virgules.

```env
# âŒ Incorrect
OLLAMA_AVAILABLE_MODELS=gemma3:4b,gemma3:12b

# âœ… Correct
OLLAMA_AVAILABLE_MODELS=["gemma3:4b","gemma3:12b"]
```

---

### âŒ Le LLM ne rÃ©pond pas / `Load failed`

```bash
# VÃ©rifier qu'Ollama tourne et que les modÃ¨les sont prÃ©sents
docker compose exec ollama ollama list

# Tester le modÃ¨le directement
docker compose exec ollama ollama run gemma3:4b "bonjour"
```

Si le modÃ¨le n'est pas dans la liste :
```bash
docker compose exec ollama ollama pull gemma3:4b
```

---

### âŒ `Docling non installÃ©` / `_DOCLING_OK = False`

```bash
docker compose exec backend python -c "
from app.services.docling_service import _DOCLING_OK
print('Docling OK:', _DOCLING_OK)
"

# VÃ©rifier le nom de classe disponible
docker compose exec backend python -c "
import docling.backend.pypdfium2_backend as b
print([x for x in dir(b) if 'Backend' in x])
"
```

Le bon nom de classe est `PyPdfiumDocumentBackend`.

---

### âŒ Connexion Safari â€” `The string did not match the expected pattern`

**Cause :** Safari valide le champ username comme un email.

**Fix :** DÃ©jÃ  corrigÃ© dans `login/page.tsx` via `noValidate` et `autoComplete="username"`. Si l'erreur persiste aprÃ¨s rebuild du frontend, vider le cache Safari.

---

### âŒ Disque plein â€” Qdrant WAL errors

```bash
# VÃ©rifier l'espace disque
df -h

# Nettoyer le cache Docker (attention : supprime les images non utilisÃ©es)
docker builder prune -af
docker image prune -af
```

> âš ï¸ Les volumes de donnÃ©es (Ollama, Qdrant, PostgreSQL) ne sont pas supprimÃ©s par ces commandes.

---

### âŒ `SSE` / rÃ©ponse vide sans erreur

**Cause :** Nginx buffÃ©rise le stream SSE.

VÃ©rifier `nginx.conf` â€” la route `/api/v1/chat/stream` doit avoir :
```nginx
proxy_buffering off;
proxy_cache off;
proxy_read_timeout 300s;
```

---

### âŒ Erreur bcrypt â€” `password cannot be longer than 72 bytes`

```bash
docker compose exec backend pip show bcrypt | grep Version
# Doit Ãªtre 4.0.1
```

Si version incorrecte â†’ vÃ©rifier `backend/requirements.txt` : `bcrypt==4.0.1`

---

---

### âŒ `nvidia-smi` â€” Driver/library version mismatch

**Cause :** Le driver Nvidia du kernel et la librairie NVML sont dÃ©synchronisÃ©s â€” 
gÃ©nÃ©ralement aprÃ¨s une mise Ã  jour du driver sans reboot.

**SymptÃ´me :**
```
Failed to initialize NVML: Driver/library version mismatch
NVML library version: 580.126
```

**Fix :**
```bash
sudo reboot
```

AprÃ¨s le reboot :
```bash
nvidia-smi                          # doit afficher la carte correctement
docker compose restart ollama
docker compose exec ollama ollama ps  # vÃ©rifier 100% GPU
```

> âš ï¸ Le reboot est obligatoire â€” le mismatch vient d'une mise Ã  jour du kernel 
> sans redÃ©marrage, laissant l'ancien driver en mÃ©moire et la nouvelle librairie sur le disque.


---

### ğŸ”„ RÃ©initialisation complÃ¨te (donnÃ©es effacÃ©es)

> âš ï¸ **Destructif** â€” supprime tous les utilisateurs, documents et vecteurs.

```bash
docker compose down -v   # -v supprime les volumes
docker compose build --no-cache
docker compose up -d
```

---

### ğŸ“‹ Checklist de diagnostic

1. `docker compose ps` â†’ tous les containers sont `Up` ?
2. `docker compose logs backend --tail=30` â†’ erreur visible ?
3. `docker compose exec ollama ollama list` â†’ modÃ¨les prÃ©sents ?
4. `docker compose exec backend python -c "from app.services.docling_service import _DOCLING_OK; print(_DOCLING_OK)"` â†’ Docling OK ?
5. Test API direct : `curl http://localhost/api/v1/auth/me` â†’ le backend rÃ©pond ?
6. Si tout est OK cÃ´tÃ© API â†’ rebuild frontend : `docker compose build --no-cache frontend && docker compose up -d frontend`
